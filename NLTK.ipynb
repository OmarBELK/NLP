{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\omar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(fileids='ca01') #  we can acess a specific file with the fileids argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.fileids()) ## how many sources this data come frome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca01',\n",
       " 'ca02',\n",
       " 'ca03',\n",
       " 'ca04',\n",
       " 'ca05',\n",
       " 'ca06',\n",
       " 'ca07',\n",
       " 'ca08',\n",
       " 'ca09',\n",
       " 'ca10']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.fileids()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20173"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.raw('cb01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assembly/nn-hl '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we access the raw file with :\n",
    "brown.raw('cb01').strip()[:15] # the first 15 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\omar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['firefox.txt',\n",
       " 'grail.txt',\n",
       " 'overheard.txt',\n",
       " 'pirates.txt',\n",
       " 'singles.txt',\n",
       " 'wine.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.words(fileids='firefox.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25 SEXY MALE, seeks attrac older single lady, for discreet encounters.\\n35YO Security Guard, seeking lady in uniform for fun times.\\n40 yo SINGLE DAD, sincere friendly DTE seeks r/ship with fem age open S/E\\n44yo tall seeks working single mum or lady below 45 fship rship. Nat Open\\n6.2 35 yr old OUTGOING M seeks fem 28-35 for o/door sports - w/e away\\nA professional business male, late 40s, 6 feet tall, slim build, well groomed, great personality, home owner, interests include the arts travel and all'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.raw('singles.txt')[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25 SEXY MALE, seeks attrac older single lady, for discreet encounters.',\n",
       " '35YO Security Guard, seeking lady in uniform for fun times.',\n",
       " '40 yo SINGLE DAD, sincere friendly DTE seeks r/ship with fem age open S/E',\n",
       " '44yo tall seeks working single mum or lady below 45 fship rship. Nat Open',\n",
       " '6.2 35 yr old OUTGOING M seeks fem 28-35 for o/door sports - w/e away',\n",
       " 'A professional business male, late 40s, 6 feet tall, slim build, well groomed, great personality, home owner, interests include the arts travel and all things good, Ringwood area, is seeking a genuine female of similar age or older, in same area or surrounds, for a meaningful long term rship. Looking forward to hearing from you all.',\n",
       " 'ABLE young man seeks, sexy older women. Phone for fun ready to play',\n",
       " 'AFFECTIONATE LADY Sought by generous guy, 40s, mutual fulfillment',\n",
       " 'ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.',\n",
       " 'AMIABLE 43 y.o. gentleman with European background, 170 cm, medium build, employed, never married, no children. Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future. 29-39 y.o. Prefer non-smoker and living in Adelaide.',\n",
       " 'ARE YOU A COPPER REDHEAD? I am 36 y.o. and looking for companionship/friendship. I enjoy the AFL, animals and dining out.',\n",
       " 'ATTRACTIVE 43 Y.O. 61, medium build, financially secure, no children, seeks attractive lady up to 40 y.o. with no children. I enjoy the beach, sports and music. View to permanent relationship.',\n",
       " 'ATTRACTIVE GUY Late 30s, 57, Taurus. Into fitness, cafes, movies, dinner parties. Seeks out there female for friendship and fun.',\n",
       " 'AUSSIE 39 Solid build, likes music, movies, walks on the beach, single, would like to meet lady for friendship view relationship.',\n",
       " 'AUSTRALIAN FATHER 58 With 11 y.o. son, living with me. Seeks nice, caring lady who likes children for permanent re lationship in coastal town on Eyre Peninsula. Age open to 55. ',\n",
       " 'AUSTRALIAN, SLIM, FUN 5 ft. 8. Enjoy BBQs, sport, fishing, going out for meal &c. Fun to be with. Looking for a com panion, aged between 35-50. Must be genuine and good fun. Possible relationship. ',\n",
       " 'BEAUTIFUL, INTELLIGENT CARING, MUSICAL Shy, late 20s, size 12, long blonde hair and likes to drink Guiness. If this is you then I want you.',\n",
       " 'BLONDE HAIR, BLUE EYES Medium build, Im in my early 30s. Am honest, caring, likes fishing, animals, golf, bike riding, TV and DVDs, quiet nights at home, kissing and cuddling up to a special person in my life. Looking for a caring, honest lady for friendship to relationship.',\n",
       " \"BUSINESSMAN 60''ish Looking for lady, non-smoker, 56 or taller with business outlook, no ties, be prepared to travel and enjoy outdoor sports (cars, boats etc.), wining/ dining. Must have s.o.h. and have modern outlook. \",\n",
       " 'CUDDLY FULL FIGURED LADY 50 plus sought by Australian gent, early 60s, financially secure, non-drinker, non smoker, for permanent relationship.',\n",
       " 'FIT ACTIVE EASYGOING 50 y.o. male, 6 ft 2, med. build, fit and healthy, fully employed no ties, non smoker, light drinker. Seeking female friend, 40s, attractive 5 ft 5+. Med. to slim build. View towards a permanent relationship. Give me a call lets have a chat.',\n",
       " 'GENUINE AND HONEST Hi Im 44 with a good sense of humour, am romantic and love drives, fishing, camping and music. Love my 2 kids. Am looking for a lady with similar interests, aged between 38-45 for friendship/ possible relationship.',\n",
       " 'GENUINE ATTRACTIVE MAN 40 y.o., no ties, secure, 5 ft. 9, slim build. I enjoy outdoors, gym, beach, country drives and quiet nights at home. Seeking an honest, caring woman, slim or med. build, who enjoys the simple things in life. Age open. ',\n",
       " 'GOOD LISTENER Well educated, tall, blue eyes, brown hair, Mid 20s, good looking, honest n/smoker. Likes movies, drives, nights in, looks not important, nor distance/ speed. Looking for someone to complete my social circles. Age open, friendship/relationship.',\n",
       " 'GREEK/AUST. MALE Early 30s, 57, slim build, non-smoker, enjoys dining out, movies, bbqs., football and quiet nights at home. Seeking slim, attractive female, 20-30 with similar interests for friend ship/relationship.',\n",
       " 'HANDSOME BUSINESSMAN Tall, slim, independant, under standing, mid 50s, looking for classy lady who wants to retain her privacy, still retain her independance and is looking for a special private relationship. ',\n",
       " 'HERE I AM PICK ME! PICK ME! Now that I have your attention Im a 35 y.o. 59 attractive guy, non-smoker/social drinker, fit and active, is seeking a slim- medium lady for friendship/ relationship. ',\n",
       " 'LIMESTONE COAST REGION Attractive, 6 fit and healthy 46 y.o. country guy. Interests incl. self sufficiencies, permaculture animals and travel would like to meet an attractive, non- smoking fun loving lady 35-45 for friendship view to permanent relationship',\n",
       " \"MALE LATE 50''s AUST Single, tall, prof. Interests: Music, theatre, dining, art, the beach and the environment. Seeking female with similar interests to share concerts, dining etc.\",\n",
       " 'OUTGOING GUY Late 30s, seeks lady, 25-50, size unimportant for fun and good times. ',\n",
       " 'RETIRED GENT Like to meet slim lady for long term caravan travel, heading Darwin in May, share driving. Must have sense of humour, non-smoker. Age open.',\n",
       " 'ROMANTIC 35 y.o. with one child. Likes dining, movies, beach, country drives, quiet nights, seeks employed 28 - 40 year old lady for relationship. Clare or Barossa region or northern suburbs.',\n",
       " 'TALL, ATTRACTIVE MALE 40s no ties, down to earth, easy going with a sense of humour, understanding, with varied interests seeks genuine female for friendship, relationship, and quality times.',\n",
       " 'VIETNAMESE MAN Single, never married, financially secure. Seeks Australian lady for genuine relationship, single mum welcome, aged up to 40.',\n",
       " \"WIDOW EARLY 60''s Self employed professional, well established, 55, slim build, UK born, many interests incl. Sports cars, sexy, young for age, looks and outlook seeks short or long term relationship with slim, petite lady, any age up to 60 years.\",\n",
       " '31 YO GUY Slim, Seeks 25-35 yo female for friendship/relationship, kids ok. Eastern Suburbs',\n",
       " '43 yo Male, blond, outgoing, genuine, friendly seeking female bet/w 30-35 yo, easygoing, friendly.',\n",
       " '50 YEAR OLD Working tall guy, Would like to meet attractive lady for long term relationship.',\n",
       " 'CENTRAL VICTORIAN single Dad with 2 teen daughters, 46 y.o. Looking for a special lady',\n",
       " 'FIT & HEALTHY 60 y.o. self emp. tradesman, NS, seeks Indian/Sri Lankan lady, NS, Olive to Dark Complexion, 48-60 y.o. for F/dship Poss. rel.',\n",
       " 'MATURE GENT. N/S. Non drinker. Looking for fship / rship with Asian or Indian lady. 35 - 50 years.',\n",
       " 'ROMANTIC Sexy country guy 58 NS seeks similar NS lady for fun and friendship.',\n",
       " 'WELL DRESSED emotionally healthy man 37 like to meet full figured woman for relationship.',\n",
       " '48 slim, shy, S/S, S/D, likes NRL, movies, outdoors, nature walks and quiet time. DtoE, SOH, seeks same 38-50, children ok. Fship, poss Rship, genuine replies only. ALA. Lots of friends, but lonely.',\n",
       " '57YRS N/S, S/D. WLTM attractive classy Lady, slim to med build for outings, weekends away and poss perm rship, if suited. Nationality and age open.',\n",
       " 'ATTRACTIVE 34yo, GSOH, S/D. WLTM at tractive prof Ladies 40-50 who are time poor from work / career and like fun times and nights in or out. Have always connected more with older Women.',\n",
       " \"E''TAINING & interesting 46yo Prof. Kind, generous and a great listener who enjoys having fun. Witty & humorous comb with caring and responsible. Enjoys movies, music, reading & being social. WLTM Lady to enjoy company with view to beginning an exciting new rship.\",\n",
       " 'GENT 57yrs of age, GSOH, N/S, S/D, 177cm. Likes movies and theatre. Looking forward to meeting Lady with sim int. Please send photo. Fship, possible rship.',\n",
       " 'GENTLEMAN is lonely, likes cooking, house work, gardening, wild life caring, and fishing. Would like to meet a lady to share this with me.',\n",
       " 'HI, SWM, med build, 54y, likes quiet nts at home, self supporting, happy nat, fun loving, likes cooking. GSOH, S/S, S/D, seeking F 45-55 for fship, poss rship, likes to pamper, c/music, r/roll, loves life, not in a hurry.',\n",
       " 'MALE 58 years old. Is there a Woman who would like to spend 1 weekend a month in Charters Towers especially country music festival weekend. If so please write. Ref 8826 ',\n",
       " 'PROFESSIONAL Male, tall, fit, 40s, new to area who enjoys walking, travel, outdoors, walks on bch, amongst many other interests. Seeking Christian Woman for fship, view to rship.',\n",
       " 'SWM 45 DtoE, honest, S/D, 178cm, 79kg. Clean cut, intelligent, natur ist seeks petite open minded red hd or brunette. Asian, eurasian welcome. Home bch drives. Fship, poss rship.',\n",
       " 'WLTM sincere, caring Lady to share life with. My age is 70 yrs. GSOH, easy going, honest, reliable, DtoE person. I like walks, the beach, fishing, the outdoors, movies, music, dining out, am a N/S, S/D. Looking for fship and a poss rship.',\n",
       " 'ASIAN LADY 25+ SOUGHT BY Aussie gent 40 for F/ship possible R/ship',\n",
       " 'ASIAN LADY sought. Kids OK as I love family. Nice & honest guy.',\n",
       " 'ASIAN LADY with GSOH sought by Businessman late 30s for f/ship r/ship',\n",
       " 'ATHLETIC tall slim gen prof 40s seeks friendly lady age open Morn Pen',\n",
       " 'Are you out there & working 35-50yo NS, SD, GSOH, DTE, Caring thats me 2.',\n",
       " 'Attractive young male seek ing casual meetings with mature ladies 30-50',\n",
       " 'BRIGHT Boy with good brain, great cook & Physique. 42, 5/11 fair.',\n",
       " 'BUSINESSMAN 60+ n/s s/d seeks lady age open for f/ship r/ship Im a good listener so lets have lunch and converse. Essendon area and inner suburbs',\n",
       " 'Bright happy positive prof male 48 fit/healthy n/s seeks lady same f/r/ship',\n",
       " 'CARING LOVING & generous, seeks petite Female up to 38.',\n",
       " 'CARING well dressed rom gent, seeks classy lady 35+ of elegance for r/ship',\n",
       " 'CASUAL discreet weekday meetings no obligation East Suburbs.',\n",
       " 'EUROPEAN Male 45, seeks married European lady for R/ship.',\n",
       " 'FIT 36yo MARRIED male seeks attr married female for discreet encounter.',\n",
       " \"GENT, 30''s, seeks attr lady, easygoing, GSOH, for long term fship/rship.\",\n",
       " 'GREEK 41yo seeks Greek lady 30 - 37 for fship & rship',\n",
       " 'GREEK GUY 46, seeking Greek lady up to 50yo',\n",
       " \"GREEK Male Late 30''s seeks Lady up to 37yrs for long term relationship\",\n",
       " 'Good Looking Male seeks larger lady for f/ship. Preferably nth subs',\n",
       " 'LIKE TO BE MISTRESS of YOUR MAN like to be treated well. Bold DTE not precious, available during the day, appreciate a generous guy? Lets Talk! (50yo)',\n",
       " 'LJ seeks JS of Bayswater Be mine.',\n",
       " 'MALE 26 tall n/s seeks female early 30s for fun times South East Suburbs.',\n",
       " 'MALE 38 seeking genuine lady s/s age open for f/ship possible r/ship',\n",
       " 'MALE 60 - burly beared seeks intimate woman for outings n/s s/d F/ston/Pen',\n",
       " \"MALE MID 30''S Looking for no strings attached fun times.\",\n",
       " 'MARRIED HANDYMAN seeks lady for casual fun any age',\n",
       " \"MARRIED MALE mid 40''s attr missing TLC, seeks lady in similar position\",\n",
       " 'MARRIED MAN 50, attrac. fit, seeks lady 40-50 to share special times.',\n",
       " 'MT ELIZA Business guy seeks gorg slim lady loves life with passion',\n",
       " 'Male 44 fit running/gym seeks slim lady n/s 40-45 f/ship LTR',\n",
       " 'Male Seeks nice girl 25 - 30 serious rship.',\n",
       " 'Man 46 attractive fit, assertive, and kind. Would like to meet attractive fit stylish female. For dinner and wine. Northcote',\n",
       " 'Married Asian likes to meet lady for good times, Call me!',\n",
       " 'PROF guy 68 GSOH casual seeks sexy lady n/s age open Mornington.',\n",
       " 'SCORPIO 47 seeks passionate woman for discreet intimate encounters',\n",
       " 'SEXY MARRIED MALE seeks married lady for discreet affair.',\n",
       " 'SINGLE DAD 44yo, 6ft, 95 kg, male, romantic, caring, sincere & honest seeks medium/slim attrac single mum 35-45 def non smoker. Knox area',\n",
       " 'SINGLE MALE 45 seeks female 40-50 for 4WD weekends away',\n",
       " 'SINGLE MALE 49 seeks Fe male for f/ship possible r/ship kids OK',\n",
       " 'SINGLE good looking 45 yo seeks 40+ lady casual f/ship & fun Married Ok',\n",
       " 'SLIM attractive 40 looking for fun pref Asian.',\n",
       " 'STERN HEADMASTER With a firm hand, seeks naughty lady',\n",
       " \"Sgl 49 yo Male 6'' ave build seeks bi curious fem for fun fship poss rship.\",\n",
       " 'Single dad. 42, East sub. 5\"9 seeks woman 30+ for f/ship relationship',\n",
       " \"TALL FAIR HAIR, mid 40''s gent, affectionate/loyal, seeks lady 35-45 - R/ship\",\n",
       " 'WANT TO BE SPOILT have fun, be sexy',\n",
       " 'Warm, natural sgl DTE 38yo, Prof. seeks similar lady, genuine fship/rship',\n",
       " 'YOUNG attractive girls under 23 sought by good looking 30s guy.',\n",
       " '39 YR OLD male DTE GSOH looking for lady fun times & outings',\n",
       " 'A BUSINESSMAN 47 seeks slim attract uni student for daytime meetings',\n",
       " 'A FRIEND, A LOVER Self employed single 47 yo romantic easy going GSOH honest reliable looking to find a friend.',\n",
       " 'ASIAN GUY, 47 prof, well presented athletic no ties, N/S seeks lady to 47 for r/ship, cuddly lady welcome.',\n",
       " 'ATTRACT 32 yo male seeks a younger 18-27 lady for f/ship relationship',\n",
       " 'CARING FIT SINGLE 52 yo Aust male. N/S GOSH financially secure seeks busty / curvaceous lady 42-48 for f/ship poss perm r/ship East suburbs.',\n",
       " 'DISCIPLINARIAN Seeks lady for fun times',\n",
       " 'Euro Guy Nth Subs seeks lady into cars, friendship, good times & family',\n",
       " 'GOOD Guy seeking Asian lady, 20-30 N/S for f/ship r/ship.',\n",
       " 'GUY SLIM FIT seeks lady, any age for intimate encounters',\n",
       " 'Genuine, caring 52yo Male seeking DTE, female 50+ poss. R/ship.',\n",
       " 'HARLEY RIDER 50 seeks casual affair with married lady.',\n",
       " 'HERPES male seeks fem under 36 for r/ship, enjoys, outdoors, movies, dinners.',\n",
       " 'JAPANESE/ASIAN 40-50 sought by Aussie mid 40s b/man f/ship r/ship',\n",
       " 'LOVE to meet widowed lady over 50, no children in North West Suburbs.',\n",
       " 'MALE 34 seeking DTE lady GSOH for casual fun, age & size open.',\n",
       " 'MARRIED 32 fit attractive travels a lot looking for women for fun',\n",
       " 'MARRIED 32 personal trainer looking for married woman age open for fun',\n",
       " 'MARRIED Dark guy 37 seeks married lady 30-48 for discreet times. Sth E Subs.',\n",
       " 'MARRIED MAN 42yo 6ft, fit, seeks Lady for discreet fun meetings.',\n",
       " \"Mid 30''s Aussie guy, 6ft 2'' olive skin, never married no kids, but I have a dog who thinks he is human. I would like to meet a sexy, honest, reliable partner who enjoys being spoilt and is ready for commitment\",\n",
       " 'OLDER LADY sought by 32 male for friendship relation ship',\n",
       " 'TALL MALE 35 European seeking fun, adventure & other things.',\n",
       " '2 NICE GUYS Aged 39 & 36 would like to meet two females for quality times, night clubbing, movies, dining out and friendship. TO',\n",
       " 'A COUNTRY BOY 50+, good soh, non smoker, social drinker, Harley rider, semi retired, secure. Looking for nice attractive lady, slim to medium build, under 55 yrs. Must be non smoker, good soh to share life and fun times with. Possible relationship.',\n",
       " 'A LITTLE MAGIC Looking for a lady, who is a non smoker, 45 - 52, slim to medium build, who likes long walks, articulate conversation, music, and a little fun with a 6 ft. 2 medium build blue/green eyed. gentleman.',\n",
       " 'ABOUND AND CAREFREE Vibrant personality, n-s, 5 ft. 11, 49 yrs. young, creative & adventurous, executive, articulate, intelligent & very flexible. Love a good laugh, love life & enjoy contrasts and the finer things in life. Seeks lady with same outlooks to 55 y.o.',\n",
       " 'ABOUT ME: 36 Easygoing, self employed, 511, medium build, smoker/social drinker, seeking slim-medium build woman who is happy in life, age open. ',\n",
       " 'ACTIVE 57 YEARS 5 ft. 8, slim build, non-smoker, social drinker, living Southern suburbs. I enjoy a healthy, active lifestyle. Interests are: Sports, travel, dining out, movies and quiet nights at home. Seeking an affectionate, slim, easy going lady to 57 years for relationship.',\n",
       " 'ADELAIDE HILLS Good looking, single, Aussie country guy, 43 y.o. med. build, 5 ft. 8., honest and sincere. Likes BBQs, fishing, camping, dining out. Seeking genuine 30-45 y.o. lady who enjoys the outdoors and country life.',\n",
       " 'ADVENTUROUS 42 y.o. sensitive, non smoker, vegetarian, slim, 511\", well travelled, beach loving. Seeks down to earth 25-43 y.o., slim, non smoker, into outdoor life, camping, walks and music. TO',\n",
       " 'AGED 36, single, outgoing, good looking, short dark hair. Interested in AFL, basketball, cricket, 10 pin bowling and enjoy SANFL, travelling, going out for dinner, movies and nightclubs.',\n",
       " '52 Y.O. TERTIARY Educated professional woman, seeks professional, employed man, with interests in theatre, dining, music, good conversation etc. ',\n",
       " 'ABOUT ME 53 y.o. lady, 5 ft 5, non smoker, social drinker. I enjoy gardening, music, movies, walking, dining out and quiet nights at home, V8 motor racing. I am an easygoing, honest and caring person, seeking gentleman 50 - 65 for friendship with view to relationship.',\n",
       " 'ATTRACTIVE EMPLOYED LADY Late 30s, seeking tall 5 ft. 10+ attractive to very attractive dark haired gentleman, strictly 37 - 41. No kids, working normal hours, not self employed, living near City.',\n",
       " \"ATTRACTIVE 50''ish LADY Easygoing, down to earth. I enjoy travelling, movies, dining, wining, weekends away. Seeking gent 58-65 with similar interests. \",\n",
       " 'ATTRACTIVE EASY GOING Intelligent, slim-medium build, 5 ft. 5, healthy living - exercise daily, employed and responsible. Seeking a trustworthy male 42-47 with integrity and good morals, non-smoker, social drinker. Children OK.',\n",
       " 'BALLROOM DANCING SEEKING DANCING PARTNER Me: Mid 60s, 55\", slimmish and after many years would like to enjoy dancing again. You must be taller than I and to avoid any complications, like me, unattached.',\n",
       " 'CITY GIRL SEEKS COUNTRY ROMANCE Attractive 38 y.o. brunette living in Adelaide seeks country farmer/land owner (36-45), pref. clean shaven, non smoker, honest with strong family values for social outings and companionship. Other callers welcome to reply.',\n",
       " 'DO YOU LOVE TO DANCE? Fun loving and employed, I enjoy gardening, BBQs, dining out, house parties &c. Looking for fit Rock N Roll dance partner. Friendship maybe leading to relationship.',\n",
       " \"EARLY 60''S Vibrant personality, good s.o.h. non-smoker/social drinker. Interests include beach walks, movies, fishing, dancing, looking for gent 510 plus. Prefer living western/southern suburbs. Friendship view to relationship.\",\n",
       " 'EASYGOING 53 Y.O. 5ft. 6 in. tall and of large build seeks a good man. I am a nonsmoker, social drinker, single mother of a 15 year old boy. Lives NE subs.',\n",
       " 'FEISTY FOXY & A YUMMY GRANDMUMMY Late 40s, working full time. Looking for love & laughter. Are you at least 59, non-smoker, adventurous enough for exciting travel & comfortable enough to just watch TV, secure, fit but not fanatical? A bonus would be a love of dancing.',\n",
       " 'GARDEN LOVER Hi! I am an Aust. lady, 62 yo, 5 ft. 1, slim-med., build, n-s, s-d. Interests: dining out, movies, entertain., quiet nights at home, walks on beach & gardening Seeks gentleman who is honest & looking for friendship, maybe lead to relationship.',\n",
       " 'SEEKING HONEST MAN I am 41 y.o., 5 ft.4, med. build, employed, fun loving lady with a good SOH. Enjoy movies, dining out, country drives & quiet times. Seeks 35-45, honest man with good SOH & similar interests, friendship poss. relation ship. Southern area.',\n",
       " 'VERY ROMANTIC LADY Attractive 54 y.o., medium build, who loves hugs and kisses and those special tingles when we click. No children but a home lover. Amongst other things I enjoy being by the river. Would like to meet a real romantic, age 50-65 y.o.',\n",
       " 'WOMAN OF SUBSTANCE 56, 59 kg., 50, fit, n-s, elegant, articulate, seeks honest, sincere, social, n-s, well preserved 6. prof, sgle. Prince Charming. Living city/inner subs to share a future filled with fun, travel, adventure, glorious sunsets, candlelight dinners, fine wine, romance & true love.',\n",
       " 'YORKE PENINSULA LADY Late 70s, 53, pleasingly plump, missing male conversation and company. My interests are: luncheons, cards, travel and indoor bowls. ',\n",
       " 'AFFECTIONATE loyal female, seeks tall male. 35-43 for genuine relationship/marriage. Melb. subs.',\n",
       " '43YO WLTM Male who enjoys life, dining in/ out, movies, long drives, car racing, gardening, travel. Must be employed, honest, and loving.',\n",
       " '48YO SWF WLTM a 42-54yo genuine, caring, honest and normal man for fship, poss rship. S/S, S/D, GSOH. Photo pls.',\n",
       " 'A tall attractive Lady, 40 yrs young looking for a Male companion to go dancing with. Could be light hearted dance classes or out and about. I have some experience in Ballroom and Jive.',\n",
       " 'LADY 51 WLTM SWM 50-57 for company and fship. S/S, S/D, enjoys movies, dining out and sedate outdoor walks. Photo appreciated.',\n",
       " 'LADY 63. WLTM a genu ine person DtoE, possibly religious, loves horses, dogs, natural food (vegetarian), beaches, reading, fish ing. Friendship please.',\n",
       " 'LADY, 43 well presented. WLTM Gent for dating, rship. Likes fishing, dining out, movies, life and wine. N/S, must be fin. secure. If you would like to meet a lovely, sexy lady please reply with a photo if poss. Genuine replies.',\n",
       " 'SLIM well presented Fem. Early 50s, outgoing, intelligent, independant, into fitness, outdoor activites, sport, beach, music, theatre, movies, read ing. WLTM N/S Male with sim. int to share outings. Poss rship.',\n",
       " 'SWF 40yo, S/D, S/S, sgl Mum. Looking to meet gen, honest, caring, non judgemental Guy up to 45yo. Must enjoy danc ing and life in general. Must be comfortable with themselves. Photo pls. Only gen enquiries.',\n",
       " 'ABBREVIATIONS GSOH Good sense of humour SOH Sense of humour N/D Non drinker S/D Social drinker S/S Social smoker N/S Non smoker ALA All letters answered WLTM Would like to meet DtoE Down to Earth DONT FORGET.. Its FREE to advertise in Perfect Match ! ',\n",
       " '']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.raw('singles.txt').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t25 SEXY MALE, seeks attrac older single lady, for discreet encounters.\n",
      "1:\t35YO Security Guard, seeking lady in uniform for fun times.\n",
      "2:\t40 yo SINGLE DAD, sincere friendly DTE seeks r/ship with fem age open S/E\n",
      "3:\t44yo tall seeks working single mum or lady below 45 fship rship. Nat Open\n",
      "4:\t6.2 35 yr old OUTGOING M seeks fem 28-35 for o/door sports - w/e away\n",
      "5:\tA professional business male, late 40s, 6 feet tall, slim build, well groomed, great personality, home owner, interests include the arts travel and all things good, Ringwood area, is seeking a genuine female of similar age or older, in same area or surrounds, for a meaningful long term rship. Looking forward to hearing from you all.\n",
      "6:\tABLE young man seeks, sexy older women. Phone for fun ready to play\n",
      "7:\tAFFECTIONATE LADY Sought by generous guy, 40s, mutual fulfillment\n",
      "8:\tARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n",
      "9:\tAMIABLE 43 y.o. gentleman with European background, 170 cm, medium build, employed, never married, no children. Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future. 29-39 y.o. Prefer non-smoker and living in Adelaide.\n",
      "10:\tARE YOU A COPPER REDHEAD? I am 36 y.o. and looking for companionship/friendship. I enjoy the AFL, animals and dining out.\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(webtext.raw('singles.txt').split('\\n')):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(str(i)+':\\t'+line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25', 'SEXY', 'MALE', ',', 'seeks', 'attrac', 'older', 'single', 'lady', ',']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.words(fileids='singles.txt')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_no8 = webtext.raw('singles.txt').split('\\n')[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to split strings to sentences we use sent_tokenize \n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n"
     ]
    }
   ],
   "source": [
    "print(single_no8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
       " 'Maybe we could explore new beginnings together?',\n",
       " 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
       " 'You WONT be disappointed.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(single_no8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
      "['Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
      "['Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
      "['You', 'WONT', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(single_no8):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
       " 'Maybe we could explore new beginnings together?',\n",
       " 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
       " 'You WONT be disappointed.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(single_no8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
      "['maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
      "['im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
      "['you', 'wont', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(single_no8):\n",
    "    print([word.lower() for word in word_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_no8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'You', 'WONT', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(single_no8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMIABLE 43 y.o.',\n",
       " 'gentleman with European background, 170 cm, medium build, employed, never married, no children.',\n",
       " 'Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future.',\n",
       " '29-39 y.o.',\n",
       " 'Prefer non-smoker and living in Adelaide.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_no9 = webtext.raw('singles.txt').split('\\n')[9]\n",
    "sent_tokenize(single_no9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AMIABLE 43 y.o. gentleman with European background, 170 cm, medium build, employed, never married, no children. Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future. 29-39 y.o. Prefer non-smoker and living in Adelaide.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.raw('singles.txt').split('\\n')[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.raw('singles.txt').split('\\n')[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_no8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'you', 'wont', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "single_no8_tokenized_lowered = list(map(str.lower,word_tokenize(single_no8)))\n",
    "print(single_no8_tokenized_lowered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to remove the stopwords using the English stopwords list in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'lost', 'r/ship', ',', 'hope', 'sight', '?', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'looking', 'someone', 'similar', '.', 'wont', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = set(stopwords.words('english'))\n",
    "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Often we want to remove the Ponctuation from the documents too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From string.punctuation :  <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print('From string.punctuation : ', type(punctuation), punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining the punctuation with the stopwords from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y', \"you'll\", 'were', \"isn't\", 'all', 'more', 'o', 'am', 'does', \"that'll\", 'm', ')', 'the', 'itself', '-', 'hers', \"wasn't\", 'out', \"aren't\", 'be', 'she', '+', 'ma', 'didn', '#', 'that', 'of', 'an', ':', \"won't\", 'ain', \"hasn't\", 'have', 'yours', 'doing', 'through', 'yourselves', 'or', 'nor', 're', ',', '.', 'no', 'don', 'each', '\\\\', '=', 'her', 'not', 'had', \"didn't\", 'but', 'about', 'i', 'in', 'most', '|', 'there', 'our', 'has', \"you're\", 'under', 'hadn', 'over', '@', 'as', 'until', 'we', \"doesn't\", 'same', 'he', '\"', 'too', '/', 'because', '%', \"'\", ';', 'myself', 'mustn', 'having', 'a', 'mightn', \"should've\", 'its', 'will', 'after', '$', 'again', 'them', 'wouldn', '<', 'ourselves', 'where', 'hasn', 'and', '(', 'to', 'should', '!', \"don't\", 'up', 'why', 'wasn', 'their', \"wouldn't\", 'here', 'theirs', \"you've\", 'off', 'such', 'now', 'this', 'needn', \"it's\", \"haven't\", 'further', 'when', 'll', \"she's\", 'are', 'once', \"weren't\", '}', 'himself', 'is', 'below', 'you', 'both', 'his', 'can', 't', \"shouldn't\", '*', 'which', 'some', 'by', \"needn't\", 'these', 'your', 'any', 'between', 'being', '{', 'themselves', 'few', 'while', \"couldn't\", 'from', 'before', 'so', 'd', '^', '[', 'isn', ']', 'down', 'how', 'shouldn', \"shan't\", 'weren', 'what', 'won', '&', '?', 'against', \"mightn't\", 'very', 's', 'they', 'at', 'was', \"you'd\", 'above', 'my', 'shan', '>', 'only', '~', 'into', 'did', 'it', 'other', 'him', 'on', 'been', 'aren', '`', 'yourself', 'then', 'herself', 've', 'just', 'me', 'doesn', 'haven', 'ours', 'do', 'than', 'couldn', 'if', 'those', 'with', \"mustn't\", '_', \"hadn't\", 'whom', 'for', 'own', 'who', 'during'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_en_with_punct = stopwords_en.union(set(punctuation))\n",
    "print(stopwords_en_with_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 211)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_en), len(stopwords_en_with_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords with punctuation from Single no 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'lost', 'r/ship', 'hope', 'sight', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'needs', 'looking', 'someone', 'similar', 'wont', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en_with_punct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a stronger/longer list of stopwords \n",
    "\n",
    "From the previous output, we have still dangly model verbs (i.e. 'could', 'wont', etc.).\n",
    "\n",
    "We can combine the stopwords we have in NLTK with other stopwords list we find online.\n",
    "\n",
    "Personally, I like to use stopword-json because it has stopwrds in 50 languages =)\n",
    "https://github.com/6/stopwords-json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords_json['en'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With combined stopwords : \n",
      "['lost', 'r/ship', 'hope', 'sight', 'explore', 'beginnings', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "stopwords_json = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "# combine them\n",
    "stoplist_combined = set.union(stopwords_json, stopwords_nltk_en, stopwords_punct)\n",
    "\n",
    "print('With combined stopwords : ')\n",
    "print([word for word in single_no8_tokenized_lowered if word not in stoplist_combined])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to map the different forms of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n",
    "\n",
    "The stemming and lemmatization process are hand-written regex rules written find the root word.\n",
    "\n",
    "Stemming: Trying to shorten a word with simple regex rules\n",
    "\n",
    "Lemmatization: Trying to find the root word with linguistics rules (with the use of regexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various stemmers and one lemmatizer in NLTK, the most common being:\n",
    "\n",
    "  - Porter Stemmer from Porter (1980)\n",
    "  - Wordnet Lemmatizer (port of the Morphy: https://wordnet.princeton.edu/man/morphy.7WN.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "port = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "walk\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(port.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n",
      "walk\n",
      "walked\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n",
      "walk\n",
      "walked\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for word in ['walking','walks','walked']:\n",
    "    print(wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't cover what's POS today so I'll just show you how to \"whip\" the lemmatizer to do what you need.\n",
    "\n",
    "By default, the WordNetLemmatizer.lemmatize() function will assume that the word is a Noun if there's no explict POS tag in the input.\n",
    "\n",
    "First you need the pos_tag function to tag a sentence and using the tag convert it into WordNet tagsets and then put it through to the WordNetLemmatizer.\n",
    "\n",
    "Note: Lemmatization won't really work on single words alone without context or knowledge of its POS tag (i.e. we need to know whether the word is a noun, verb, adjective, adverb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
    "                  'VB': 'v', 'RB': 'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\omar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('he', 'PRP'), ('is', 'VBZ'), ('walking', 'VBG'), ('to', 'TO'), ('school', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# `pos_tag` takes the tokenized sentence as input, i.e. list of string,\n",
    "# and returns a tuple of (word, tg), i.e. list of tuples of strings\n",
    "# so we need to get the tag from the 2nd element.\n",
    "\n",
    "#from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "walking_tagged = pos_tag(word_tokenize('he is walking to school'))\n",
    "print(walking_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "v\n",
      "v\n",
      "n\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "for _, tag in walking_tagged:\n",
    "    print(penn2morphy(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'be', 'walk', 'to', 'school']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, lets create a new lemmatization function for sentences given what we learnt above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'be', 'walk', 'to', 'school']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB': 'r'}\n",
    "    try :\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "def lemmatize_sent(text):\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
    "                                   for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "print(lemmatize_sent('he is walking to school'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Single no : \n",
      "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed. \n",
      "\n",
      "Lemmatized and removed stopwords: \n",
      "['lose', 'r/ship', 'hope', 'sight', 'explore', 'beginning', 'im', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappoint']\n"
     ]
    }
   ],
   "source": [
    "print('Original Single no : ')\n",
    "print(single_no8, '\\n')\n",
    "print('Lemmatized and removed stopwords: ')\n",
    "print([word for word in lemmatize_sent(single_no8) if word not in stoplist_combined and not word.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining what we know about removing stopwords and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    print([word for word in lemmatize_sent(text) if word not in stoplist_combined and not word.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tangential Note on Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Strings to Vectors \n",
    "\n",
    "Vector Space Model is conceptualizing language as a whole lot of numbers\n",
    "\n",
    "Bag-of-Words (BoW): Counting each document/sentence as a vector of numbers, with each number representing the count of a word in the corpus\n",
    "\n",
    "To count, we can use the Python collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox\"\n",
    "\n",
    "\n",
    "# lemmatize and remove stopwords\n",
    "processed_sent1 = lemmatize_sent(sent1)\n",
    "processed_sent2 = lemmatize_sent(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Sentence:\n",
      "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'brown', 'dog']\n",
      "\n",
      "Word Counts:\n",
      "Counter({'the': 2, 'brown': 2, 'quick': 1, 'fox': 1, 'jump': 1, 'over': 1, 'lazy': 1, 'dog': 1})\n"
     ]
    }
   ],
   "source": [
    "print('Processed Sentence:')\n",
    "print(processed_sent1)\n",
    "print()\n",
    "print('Word Counts:')\n",
    "print(Counter(processed_sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Sentence:\n",
      "['mr', 'brown', 'jump', 'over', 'the', 'lazy', 'fox'] \n",
      "\n",
      "Word Counts:\n",
      "Counter({'mr': 1, 'brown': 1, 'jump': 1, 'over': 1, 'the': 1, 'lazy': 1, 'fox': 1})\n"
     ]
    }
   ],
   "source": [
    "print('Processed Sentence:')\n",
    "print(processed_sent2, '\\n')\n",
    "print('Word Counts:')\n",
    "print(Counter(processed_sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = [2,1,1,1,1,1,0]\n",
    "sent2 = [1,0,1,1,1,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit_transform(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 8,\n",
       " 'quick': 7,\n",
       " 'brown': 0,\n",
       " 'fox': 2,\n",
       " 'jumps': 3,\n",
       " 'over': 6,\n",
       " 'lazy': 4,\n",
       " 'dog': 1,\n",
       " 'mr': 5}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check the vocabulary in our vectorizer\n",
    "# It's a dictionary where the words are the keys and \n",
    "# The values are the IDs given to each word. \n",
    "\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We haven't counted anything yet just initializing our vectorizer object with the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't tell the vectorizer to remove punctuation and tokenize and lowercase, how did they do it?\n",
    "\n",
    "Also, the is in the vocabulary, it's a stopword, we want it gone...\n",
    "And jumps isn't stemmed or lemmatized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Achso, we can override these arguments with the functions we have learnt before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omar\\miniconda3\\envs\\tensorenv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '``', 'ai', 'ca', \"n't\", 'sha', 'wo'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    count_vect = CountVectorizer(stop_words=stoplist_combined, tokenizer= word_tokenize)\n",
    "    count_vect.fit_transform(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quick': 6, 'brown': 0, 'fox': 2, 'jumps': 3, 'lazy': 4, 'dog': 1, 'mr': 5}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u can see the difference. \n",
    "{'the': 8, 'quick': 7, 'brown': 0, 'fox': 2, 'jumps': 3, 'over': 6, 'lazy': 4, 'dog': 1, 'mr': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quick': 6, 'brown': 0, 'fox': 2, 'jump': 3, 'lazy': 4, 'dog': 1, 'mr': 5}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    # Override the analyzer totally with our preprocess text\n",
    "    count_vect = CountVectorizer(analyzer=preprocess_text)\n",
    "    count_vect.fit_transform(fin)\n",
    "    \n",
    "count_vect.vocabulary_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To vectorize any new sentences\n",
    "we use CountVectorizer.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.transform([sent1, sent2])\n",
    "# this results in a sparce matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quick': 6, 'brown': 0, 'fox': 2, 'jump': 3, 'lazy': 4, 'dog': 1, 'mr': 5}\n",
      "dict_items([('quick', 6), ('brown', 0), ('fox', 2), ('jump', 3), ('lazy', 4), ('dog', 1), ('mr', 5)])\n",
      "[('brown', 0), ('dog', 1), ('fox', 2), ('jump', 3), ('lazy', 4), ('mr', 5), ('quick', 6)]\n",
      "[('brown', 0), ('dog', 1), ('fox', 2), ('jump', 3), ('lazy', 4), ('mr', 5), ('quick', 6)]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "print(count_vect.vocabulary_)\n",
    "print(count_vect.vocabulary_.items())\n",
    "print(sorted(count_vect.vocabulary_.items()))\n",
    "print(sorted(count_vect.vocabulary_.items(), key = itemgetter(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
      "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
      "\n",
      "vocab:  ('brown', 'dog', 'fox', 'jump', 'lazy', 'mr', 'quick')\n",
      "\n",
      "Matrix/vectors: \n",
      " [[2 1 1 1 1 0 1]\n",
      " [1 0 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "words_sorted_by_index, _ = zip(*sorted(count_vect.vocabulary_.items(), key = itemgetter(1)))\n",
    "print(preprocess_text(sent1))\n",
    "print(preprocess_text(sent2))\n",
    "print()\n",
    "print('vocab: ',words_sorted_by_index)\n",
    "print()\n",
    "print('Matrix/vectors: \\n', count_vect.transform([sent1, sent2]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data.\n",
    "\n",
    "The task is to create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('D:/Machine_Learning_Data/train.json') as fin:\n",
    "    trainjson = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4040"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 0,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 1,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_l25d7',\n",
       " 'request_number_of_comments_at_retrieval': 0,\n",
       " 'request_text': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_text_edit_aware': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_title': 'Request Colorado Springs Help Us Please',\n",
       " 'requester_account_age_in_days_at_request': 0.0,\n",
       " 'requester_account_age_in_days_at_retrieval': 792.4204050925925,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 792.4204050925925,\n",
       " 'requester_number_of_comments_at_request': 0,\n",
       " 'requester_number_of_comments_at_retrieval': 0,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 0,\n",
       " 'requester_number_of_posts_at_request': 0,\n",
       " 'requester_number_of_posts_at_retrieval': 1,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 0,\n",
       " 'requester_received_pizza': False,\n",
       " 'requester_subreddits_at_request': [],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 1,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 1,\n",
       " 'requester_user_flair': None,\n",
       " 'requester_username': 'nickylvst',\n",
       " 'unix_timestamp_of_request': 1317852607.0,\n",
       " 'unix_timestamp_of_request_utc': 1317849007.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainjson[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID:\t t3_l25d7 \n",
      "\n",
      "Title:\t Request Colorado Springs Help Us Please \n",
      "\n",
      "Text:\t Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated \n",
      "\n",
      "Tag:\t False\n"
     ]
    }
   ],
   "source": [
    "print('UID:\\t', trainjson[0]['request_id'], '\\n')\n",
    "print('Title:\\t', trainjson[0]['request_title'], '\\n')\n",
    "print('Text:\\t', trainjson[0]['request_text_edit_aware'], '\\n')\n",
    "print('Tag:\\t', trainjson[0]['requester_received_pizza'], end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omar\\miniconda3\\envs\\tensorenv\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "## here is a neat trick to convert a json to pandas DataFrame\n",
    "import pandas as pd\n",
    "df = pd.io.json.json_normalize(trainjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4040, 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_title</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lpu5j</td>\n",
       "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_mxvj3</td>\n",
       "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_1i6486</td>\n",
       "      <td>[Request] Old friend coming to visit. Would LO...</td>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id                                      request_title  \\\n",
       "0   t3_l25d7            Request Colorado Springs Help Us Please   \n",
       "1   t3_rcb83  [Request] California, No cash and I could use ...   \n",
       "2   t3_lpu5j  [Request] Hungry couple in Dundee, Scotland wo...   \n",
       "3   t3_mxvj3  [Request] In Canada (Ontario), just got home f...   \n",
       "4  t3_1i6486  [Request] Old friend coming to visit. Would LO...   \n",
       "\n",
       "                             request_text_edit_aware  requester_received_pizza  \n",
       "0  Hi I am in need of food for my 4 children we a...                     False  \n",
       "1  I spent the last money I had on gas today. Im ...                     False  \n",
       "2  My girlfriend decided it would be a good idea ...                     False  \n",
       "3  It's cold, I'n hungry, and to be completely ho...                     False  \n",
       "4  hey guys:\\n I love this sub. I think it's grea...                     False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we choose just the needed Features\n",
    "df_train = df[['request_id', 'request_title', \n",
    "               'request_text_edit_aware', \n",
    "               'requester_received_pizza']]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('D:/Machine_Learning_Data/test.json') as fin:\n",
    "    testjson = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1631, 4040)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testjson), len(trainjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID :\t t3_i8iy4 \n",
      "\n",
      "Title:\t [request] pregger gf 95 degree house and no food.. promise to pay it forward! Northern Colorado \n",
      "\n",
      "Text:\t Hey all! It's about 95 degrees here and our kitchen is pretty much empty save for some bread and cereal.  My girlfriend/fiance is 8 1/2 months pregnant and we could use a good meal.  We promise to pay it forward when we get money! Thanks so much in advance! \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'requester_received_pizza'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-365f6f04bafd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Title:\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'request_title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Text:\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'request_text_edit_aware'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tag:\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'requester_received_pizza'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'requester_received_pizza'"
     ]
    }
   ],
   "source": [
    "print('UID :\\t', testjson[0]['request_id'],'\\n')\n",
    "print('Title:\\t', testjson[0]['request_title'], '\\n')\n",
    "print('Text:\\t', testjson[0]['request_text_edit_aware'], '\\n')\n",
    "print('Tag:\\t', testjson[0]['requester_received_pizza'], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test data, our label (i.e. requester_received_pizza) won't be known to us since that's the thing that our classifier is predicting.\n",
    "\n",
    "Note: Whatever features that we're going to train our classifier with, we should have them in our test set too. In our case we need to make sure that the test set has request_text_edit_aware field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omar\\miniconda3\\envs\\tensorenv\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_title</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>[request] pregger gf 95 degree house and no fo...</td>\n",
       "      <td>Hey all! It's about 95 degrees here and our ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>[Request] Lost my job day after labour day, st...</td>\n",
       "      <td>I didn't know a place like this exists! \\n\\nI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>(Request) pizza for my kids please?</td>\n",
       "      <td>Hi Reddit. Im a single dad having a really rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>[Request] Just moved to a new state(Waltham MA...</td>\n",
       "      <td>Hi I just moved to Waltham MA from my home sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>[Request] Two girls in between paychecks, we'v...</td>\n",
       "      <td>We're just sitting here near indianapolis on o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id                                      request_title  \\\n",
       "0   t3_i8iy4  [request] pregger gf 95 degree house and no fo...   \n",
       "1  t3_1mfqi0  [Request] Lost my job day after labour day, st...   \n",
       "2   t3_lclka                (Request) pizza for my kids please?   \n",
       "3  t3_1jdgdj  [Request] Just moved to a new state(Waltham MA...   \n",
       "4   t3_t2qt4  [Request] Two girls in between paychecks, we'v...   \n",
       "\n",
       "                             request_text_edit_aware  \n",
       "0  Hey all! It's about 95 degrees here and our ki...  \n",
       "1  I didn't know a place like this exists! \\n\\nI ...  \n",
       "2  Hi Reddit. Im a single dad having a really rou...  \n",
       "3  Hi I just moved to Waltham MA from my home sta...  \n",
       "4  We're just sitting here near indianapolis on o...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.io.json.json_normalize(testjson) ## Pandas magic\n",
    "df_test = df[['request_id','request_title','request_text_edit_aware']]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training data before vectorization\n",
    "\n",
    "The first thing to do is to split our training data into 2 parts:\n",
    "\n",
    "- training: Use for training our model\n",
    "- validation: Use to check the \"soundness\" of our model\n",
    "\n",
    "Note:\n",
    "\n",
    "Splitting the data into 2 parts and holding out one part to check the model is one of method to validate the \"soundness\" of our model. It's call the hold-out validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(df_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3232, 4), (808, 4))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer and \n",
    "# override the analyzer totally with the preprocess_text().\n",
    "# Note: the vectorizer is just an 'empty' object now.\n",
    "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
    "\n",
    "# When we use `CounterVectorizer.fit_transform`,\n",
    "# we essentially create the dictionary and \n",
    "# vectorize our input text at the same time.\n",
    "train_set = count_vect.fit_transform(train['request_text_edit_aware'])\n",
    "train_tags = train['requester_received_pizza']\n",
    "\n",
    "# When vectorizing the validation data, we use `CountVectorizer.transform()`.\n",
    "valid_set = count_vect.transform(valid['request_text_edit_aware'])\n",
    "valid_tags = valid['requester_received_pizza']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we need to vectorize the test data too\n",
    "After we vectorize our data, the input to train the classifier would be the vectorized text.\n",
    "When we predict the label with the trained mdoel, our input needs to be vectorized too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = count_vect.transform(df_test['request_text_edit_aware'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier in sklearn\n",
    "\n",
    "There are different variants of Naive Bayes (NB) classifier in sklearn.\n",
    "For simplicity, lets just use the MultinomialNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_set, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza reception accuracy = 74.5049504950495\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "valid_pred = nb_clf.predict(valid_set)\n",
    "print('Pizza reception accuracy = {}'.format(accuracy_score(valid_pred, valid_tags)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets use the full training data set and re-vectorize and retrain the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer = preprocess_text)\n",
    "\n",
    "full_train_set = count_vect.fit_transform(df_train['request_text_edit_aware'])\n",
    "full_tags = df_train['requester_received_pizza']\n",
    "# Note: We have to re-vectorize the test set since\n",
    "#       now our vectorizer is different using the full \n",
    "#       training set.\n",
    "test_set = count_vect.transform(df_test['request_text_edit_aware'])\n",
    "\n",
    "## Train the Model\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(full_train_set, full_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we use the classifier to predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb_clf.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note: Since we don't have the requester_received_pizza field in test data, we can't measure accuracy. But we can do some exploration as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 4040 requests, only 994 gets their pizzas,24.603960396039604% success rate\n"
     ]
    }
   ],
   "source": [
    "success_rate = sum(df_train['requester_received_pizza']) / len(df_train) * 100\n",
    "print(str('Of {} requests, only {} gets their pizzas,'\n",
    "         '{}% success rate'.format(len(df_train), sum(df_train['requester_received_pizza']),\n",
    "                                  success_rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Classifier is rather Stingy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 1631, only 56 ,gets their pizza 3.4334763948497855% success rate\n"
     ]
    }
   ],
   "source": [
    "success_rate = sum(predictions)/len(predictions)*100\n",
    "print(str('Of {}, only {} ,gets their pizza '\n",
    "         '{}% success rate'.format(len(predictions), sum(predictions), success_rate)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorenv",
   "language": "python",
   "name": "tensorenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
